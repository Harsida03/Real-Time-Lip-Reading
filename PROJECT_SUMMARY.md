# Audio-Visual Lip Reading System - Complete Project Summary

## Table of Contents
1. [Project Overview](#project-overview)
2. [System Architecture](#system-architecture)
3. [Technology Stack](#technology-stack)
4. [Complete System Flow](#complete-system-flow)
5. [Core Components](#core-components)
6. [Visual Speech Recognition Pipeline](#visual-speech-recognition-pipeline)
7. [Audio Integration](#audio-integration)
8. [Fusion Algorithm](#fusion-algorithm)
9. [Usage & Deployment](#usage--deployment)
10. [Limitations & Future Work](#limitations--future-work)

---

## Project Overview

This is an **enhanced real-time multimodal speech recognition system** that combines:
- **Visual Speech Recognition (VSR)**: Deep learning-based lip-reading using webcam video
- **Automatic Speech Recognition (ASR)**: Audio-based speech recognition using microphone
- **Intelligent Fusion**: Weighted confidence-based algorithm to combine both modalities
- **LLM Post-Processing**: Language model correction for final output

### Problem Statement
Traditional lip-reading systems suffer from **viseme confusion** - words with identical or similar lip movements are indistinguishable:
- "bat" vs "pat" vs "mat" (bilabial plosives)
- "fan" vs "van" (labiodental fricatives)
- "see" vs "she" (sibilants)

### Solution
By integrating audio input, the system can:
1. Detect subtle phonetic differences invisible in visual modality
2. Use audio to disambiguate when visual predictions are uncertain
3. Fall back to visual when audio is noisy or unavailable
4. Achieve higher accuracy than either modality alone

---

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        INPUT LAYER                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   ðŸ“¹ Video Capture         â”‚   ðŸŽ¤ Audio Capture                 â”‚
â”‚   - Webcam @ 25 fps        â”‚   - Microphone @ 16kHz             â”‚
â”‚   - 640x480 resolution     â”‚   - Mono channel                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                              â”‚
             â–¼                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VISUAL PROCESSING         â”‚ â”‚  AUDIO PROCESSING              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Face Detection          â”‚ â”‚ 1. Audio Buffering             â”‚
â”‚    - MediaPipe/RetinaFace  â”‚ â”‚    - Real-time capture         â”‚
â”‚ 2. Landmark Detection      â”‚ â”‚ 2. Speech Recognition          â”‚
â”‚    - 68-point facial       â”‚ â”‚    - Google Speech API         â”‚
â”‚ 3. Mouth ROI Extraction    â”‚ â”‚ 3. Confidence Scoring          â”‚
â”‚    - 96x96 grayscale patch â”‚ â”‚    - Default: 0.7              â”‚
â”‚ 4. Temporal Processing     â”‚ â”‚                                â”‚
â”‚    - Landmark smoothing    â”‚ â”‚                                â”‚
â”‚ 5. Affine Transform        â”‚ â”‚                                â”‚
â”‚    - Normalize orientation â”‚ â”‚                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                                â”‚
             â–¼                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VISUAL MODEL INFERENCE    â”‚ â”‚  AUDIO TRANSCRIPTION           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ - ESPnet Transformer       â”‚ â”‚ - API-based ASR                â”‚
â”‚ - Pre-trained on LRS3      â”‚ â”‚ - Real-time transcription      â”‚
â”‚ - Character-level output   â”‚ â”‚ - Word-level output            â”‚
â”‚ - Beam search decoding     â”‚ â”‚                                â”‚
â”‚ - Language model rescore   â”‚ â”‚                                â”‚
â”‚                            â”‚ â”‚                                â”‚
â”‚ Output: "I SAW A BAT"      â”‚ â”‚ Output: "I saw a pat"          â”‚
â”‚ Confidence: 0.5            â”‚ â”‚ Confidence: 0.7                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                                â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚   FUSION ALGORITHM         â”‚
             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
             â”‚ 1. Similarity Calculation  â”‚
             â”‚    - Jaccard Index         â”‚
             â”‚ 2. Weighted Scoring        â”‚
             â”‚    - Audio: 60%            â”‚
             â”‚    - Visual: 40%           â”‚
             â”‚ 3. Decision Logic          â”‚
             â”‚    - High agreement: Best  â”‚
             â”‚    - Medium: Word-level    â”‚
             â”‚    - Low: Fallback         â”‚
             â”‚                            â”‚
             â”‚ Output: "I saw a pat"      â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚   LLM POST-PROCESSING      â”‚
             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
             â”‚ - Ollama (LLaMA 3.2)       â”‚
             â”‚ - Grammar correction       â”‚
             â”‚ - Capitalization           â”‚
             â”‚ - Punctuation              â”‚
             â”‚                            â”‚
             â”‚ Output: "I saw a pat."     â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚   DISPLAY OUTPUT           â”‚
             â”‚   - Visual prediction      â”‚
             â”‚   - Audio prediction       â”‚
             â”‚   - Fused result           â”‚
             â”‚   - Final corrected text   â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Technology Stack

### Deep Learning & AI
| Component | Technology | Purpose |
|-----------|-----------|---------|
| Visual Speech Recognition | **PyTorch** | Neural network framework |
| Pre-trained VSR Model | **ESPnet Transformer** | Trained on LRS3 dataset (433 hours) |
| Model Architecture | **Transformer Encoder-Decoder** | Sequence-to-sequence learning |
| Language Model | **RNN-LM** | Beam search rescoring |
| Post-processing | **Ollama (LLaMA 3.2)** | Text correction and formatting |

### Computer Vision
| Component | Technology | Purpose |
|-----------|-----------|---------|
| Video Capture | **OpenCV** | Webcam input @ 25fps |
| Face Detection | **MediaPipe / RetinaFace** | Detect faces in frames |
| Landmark Detection | **68-point facial landmarks** | Mouth region localization |
| Video Processing | **PyAV** | Video encoding/decoding |
| Image Processing | **scikit-image, scipy** | Affine transforms, cropping |

### Audio Processing
| Component | Technology | Purpose |
|-----------|-----------|---------|
| Audio Capture | **sounddevice** | Microphone input @ 16kHz |
| Speech Recognition | **Google Speech API** | Audio-to-text conversion |
| Audio I/O | **PortAudio** | System-level audio library |
| Signal Processing | **NumPy** | Audio buffering and processing |

### Configuration & Infrastructure
| Component | Technology | Purpose |
|-----------|-----------|---------|
| Configuration Management | **Hydra** | Model config & hyperparameters |
| Data Validation | **Pydantic** | Model output validation |
| User Interface | **keyboard** | Hotkey controls (Alt, Q) |

### Custom Algorithms
| Component | Technology | Purpose |
|-----------|-----------|---------|
| Fusion Algorithm | **Custom Python** | Multimodal integration |
| Confidence Scoring | **Weighted average** | Modality reliability |
| Similarity Metric | **Jaccard Index** | Text comparison |

---

## Complete System Flow

### 1. Input Capture Phase
```python
# Video Thread
webcam â†’ 640x480 RGB frames @ 25fps â†’ video buffer

# Audio Thread  
microphone â†’ 16kHz mono audio â†’ audio buffer
```

### 2. Visual Processing Pipeline
```python
# Frame Processing
for each frame:
    1. Detect face â†’ MediaPipe/RetinaFace
    2. Extract 68 landmarks â†’ facial keypoints
    3. Focus on mouth region â†’ landmarks 48-68
    4. Apply temporal smoothing â†’ window_margin=12 frames
    5. Affine transformation â†’ normalize to reference face
    6. Crop 96x96 patch â†’ mouth ROI
    7. Convert to grayscale
    
# Sequence Formation
video_sequence = stack(processed_frames)  # Shape: [T, 96, 96, 1]
```

### 3. Visual Model Inference
```python
# Data Loading
video_tensor = VideoTransform(video_sequence)  # Normalize, resize

# Model Forward Pass
encoder_output = transformer_encoder(video_tensor)
decoder_output = beam_search_decoder(
    encoder_output,
    beam_size=40,
    lm_weight=0.3,
    ctc_weight=0.1
)

# Output
visual_text = "I SAW A BAT"
visual_confidence = 0.5  # Default (not extracted yet)
```

### 4. Audio Processing
```python
# Audio Capture
audio_buffer = record_audio(duration=recording_time)

# Speech Recognition
recognizer = sr.Recognizer()
audio_text = recognizer.recognize_google(audio_buffer)
audio_confidence = 0.7  # Default from API
```

### 5. Fusion Decision
```python
# Calculate Similarity
visual_words = ["i", "saw", "a", "bat"]
audio_words = ["i", "saw", "a", "pat"]
similarity = jaccard_similarity(visual_words, audio_words)  # 0.6

# Weighted Scores
audio_score = 0.7 * 0.6 = 0.42
visual_score = 0.5 * 0.4 = 0.20

# Decision Logic
if similarity > 0.8:  # High agreement
    result = highest_weighted_score_prediction
elif similarity > 0.3:  # Medium agreement
    result = word_level_fusion(visual_words, audio_words, scores)
else:  # Low agreement
    result = fallback_to_highest_score

# Output
fused_text = "i saw a pat"
```

### 6. LLM Post-Processing
```python
# Ollama API Call
prompt = f"Correct this text: {fused_text}"
corrected = ollama.generate(model="llama3.2", prompt=prompt)

# Output
final_text = "I saw a pat."
```

---

## Core Components

### Directory Structure
```
project_root/
â”œâ”€â”€ pipelines/                      # Visual speech recognition
â”‚   â”œâ”€â”€ pipeline.py                 # Main inference pipeline
â”‚   â”œâ”€â”€ model.py                    # AVSR Transformer model
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ data_module.py          # Data loading & preprocessing
â”‚   â”‚   â”œâ”€â”€ audio_process.py        # Audio transforms
â”‚   â”‚   â””â”€â”€ video_process.py        # Video transforms
â”‚   â””â”€â”€ detectors/
â”‚       â”œâ”€â”€ mediapipe/              # MediaPipe face detection
â”‚       â”‚   â”œâ”€â”€ detector.py
â”‚       â”‚   â””â”€â”€ video_process.py
â”‚       â””â”€â”€ retinaface/             # RetinaFace detection
â”‚           â”œâ”€â”€ detector.py
â”‚           â””â”€â”€ video_process.py
â”‚
â”œâ”€â”€ espnet/                         # ESPnet framework
â”‚   â””â”€â”€ nets/
â”‚       â””â”€â”€ pytorch_backend/
â”‚           â”œâ”€â”€ e2e_asr_transformer.py      # Transformer ASR
â”‚           â””â”€â”€ e2e_asr_transformer_av.py   # Audio-Visual ASR
â”‚
â”œâ”€â”€ benchmarks/                     # Pre-trained models
â”‚   â””â”€â”€ LRS3/
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â””â”€â”€ LRS3_V_WER19.1/
â”‚       â”‚       â”œâ”€â”€ model.pth       # 1.77 GB model weights
â”‚       â”‚       â””â”€â”€ model.json      # Model configuration
â”‚       â””â”€â”€ language_models/
â”‚           â””â”€â”€ lm_en_subword/
â”‚               â”œâ”€â”€ model.pth       # RNN language model
â”‚               â””â”€â”€ model.json
â”‚
â”œâ”€â”€ configs/                        # Model configurations
â”‚   â”œâ”€â”€ LRS3_V_WER19.1.ini         # Visual-only config
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ hydra_configs/                  # Runtime configurations
â”‚   â””â”€â”€ default.yaml               # Default settings
â”‚
â”œâ”€â”€ fusion.py                       # Audio-visual fusion algorithm
â”œâ”€â”€ audio_module.py                 # Audio capture & ASR
â”œâ”€â”€ main_with_audio.py             # Enhanced main application
â”œâ”€â”€ main.py                         # Original visual-only system
â””â”€â”€ test_fusion.py                  # Fusion demo (no hardware)
```

---

## Visual Speech Recognition Pipeline

### InferencePipeline Class
**Location**: `pipelines/pipeline.py`

The core visual speech recognition system built on ESPnet framework.

#### Key Components:

1. **Configuration Loading**
   - Reads `.ini` config files (e.g., `LRS3_V_WER19.1.ini`)
   - Specifies model paths, hyperparameters, decode settings

2. **Data Loader (AVSRDataLoader)**
   - Supports modalities: `video`, `audio`, `audiovisual`
   - Video processing: face detection â†’ landmark extraction â†’ ROI crop
   - Audio processing: waveform normalization â†’ feature extraction
   - Temporal synchronization for audio-visual alignment

3. **Face Detection & Tracking**
   - **MediaPipe**: Lightweight, CPU-friendly
   - **RetinaFace**: More accurate, GPU-accelerated
   - Outputs 68 facial landmarks per frame

4. **Video Preprocessing**
   ```python
   # Per-frame processing
   1. Detect face and extract landmarks
   2. Interpolate missing landmarks (if face not detected)
   3. Temporal smoothing (window_margin=12 frames)
   4. Affine transformation to normalize face orientation
   5. Crop 96x96 mouth region centered on lips
   6. Convert to grayscale
   7. Stack frames into temporal sequence
   ```

5. **AVSR Model**
   - **Architecture**: Transformer Encoder-Decoder
   - **Pre-training**: LRS3 dataset (433 hours of lip-reading data)
   - **Word Error Rate**: 19.1% on LRS3 test set
   - **Output**: Character-level predictions
   - **Decoding**: Beam search with language model rescoring

6. **Inference Process**
   ```python
   def forward(video_file, landmarks):
       # Load and preprocess
       video_data = dataloader.load_data(video_file, landmarks)
       
       # Encode visual features
       encoder_output = model.encode(video_data)
       
       # Beam search decoding
       hypotheses = beam_search_decoder(
           encoder_output,
           beam_size=40,
           ctc_weight=0.1,
           lm_weight=0.3
       )
       
       # Select best hypothesis
       prediction = hypotheses[0]
       return prediction
   ```

### Model Configuration
**File**: `configs/LRS3_V_WER19.1.ini`

```ini
[input]
modality=video          # Visual-only mode
v_fps=25               # Video frame rate

[model]
v_fps=25
model_path=benchmarks/LRS3/models/LRS3_V_WER19.1/model.pth
model_conf=benchmarks/LRS3/models/LRS3_V_WER19.1/model.json
rnnlm=benchmarks/LRS3/language_models/lm_en_subword/model.pth
rnnlm_conf=benchmarks/LRS3/language_models/lm_en_subword/model.json

[decode]
beam_size=40           # Beam search width
penalty=0.0
ctc_weight=0.1        # CTC loss weight
lm_weight=0.3         # Language model weight
```

---

## Audio Integration

### AudioCapture Class
**Location**: `audio_module.py`

Real-time audio capture and speech recognition module.

#### Features:
- **Microphone Input**: Uses `sounddevice` for low-latency capture
- **Sample Rate**: 16kHz mono channel
- **Speech Recognition**: Google Speech API (free tier)
- **Threading**: Non-blocking audio recording
- **Buffer Management**: Queue-based audio data storage

#### API:
```python
audio_capture = AudioCapture(sample_rate=16000, channels=1)

# Start recording
audio_capture.start_recording()

# ... perform other tasks ...

# Stop and get transcription
audio_text = audio_capture.stop_recording_and_recognize()
# Returns: "I saw a pat"
```

#### Implementation Details:
```python
def audio_callback(indata, frames, time, status):
    """Called for each audio block"""
    self.audio_queue.put(indata.copy())
    self.audio_data.append(indata.copy())

def recognize(self, audio_data):
    """Convert audio to text"""
    audio_segment = np.concatenate(audio_data)
    audio_int16 = (audio_segment * 32767).astype(np.int16)
    
    audio = sr.AudioData(
        audio_int16.tobytes(),
        sample_rate=self.sample_rate,
        sample_width=2
    )
    
    text = self.recognizer.recognize_google(audio)
    return text
```

---

## Fusion Algorithm

### AudioVisualFusion Class
**Location**: `fusion.py`

Intelligent multimodal fusion using weighted confidence scores and similarity-based decision making.

#### Configuration:
```python
fusion = AudioVisualFusion(
    audio_weight=0.6,    # 60% weight to audio
    visual_weight=0.4    # 40% weight to visual
)
```

#### Algorithm:

```python
def fuse_predictions(visual_text, audio_text, 
                     visual_confidence=0.5, 
                     audio_confidence=0.7):
    
    # 1. Handle edge cases
    if not audio_text:
        return visual_text, "visual_only"
    if not visual_text:
        return audio_text, "audio_only"
    
    # 2. Calculate similarity (Jaccard Index)
    visual_words = visual_text.lower().split()
    audio_words = audio_text.lower().split()
    
    intersection = len(set(visual_words) âˆ© set(audio_words))
    union = len(set(visual_words) âˆª set(audio_words))
    similarity = intersection / union
    
    # 3. Compute weighted scores
    audio_score = audio_confidence Ã— 0.6
    visual_score = visual_confidence Ã— 0.4
    
    # 4. Decision logic based on similarity
    if similarity > 0.8:  # HIGH AGREEMENT
        # Both modalities mostly agree
        # â†’ Choose prediction with higher weighted score
        return audio_text if audio_score > visual_score else visual_text
        
    elif similarity > 0.3:  # MEDIUM AGREEMENT
        # Partial overlap, some disagreement
        # â†’ Word-level fusion
        fused = []
        for i in range(max(len(visual_words), len(audio_words))):
            if visual_words[i] == audio_words[i]:
                fused.append(audio_words[i])  # Agreement
            else:
                # Disagreement â†’ choose based on modality score
                fused.append(
                    audio_words[i] if audio_score > visual_score 
                    else visual_words[i]
                )
        return ' '.join(fused)
        
    else:  # LOW AGREEMENT (< 0.3)
        # Complete disagreement
        # â†’ Fallback to more reliable modality
        return audio_text if audio_score > visual_score else visual_text
```

#### Example Scenarios:

**Scenario 1: Viseme Confusion (Medium Agreement)**
```python
visual = "I saw a bat"
audio = "I saw a pat"

similarity = 3/4 = 0.75  # Medium
audio_score = 0.7 Ã— 0.6 = 0.42
visual_score = 0.5 Ã— 0.4 = 0.20

# Word-level fusion:
# "i" == "i" â†’ "i"
# "saw" == "saw" â†’ "saw"
# "a" == "a" â†’ "a"
# "bat" != "pat" â†’ "pat" (audio_score > visual_score)

result = "i saw a pat"  âœ“
```

**Scenario 2: High Agreement**
```python
visual = "the cat sat on the mat"
audio = "the cat sat on the mat"

similarity = 1.0  # High agreement
# Both predictions identical â†’ use higher scored modality
result = "the cat sat on the mat"  âœ“
```

**Scenario 3: Low Agreement (Noisy Conditions)**
```python
visual = "hello world"
audio = "goodbye moon"

similarity = 0.0  # No overlap
audio_score = 0.42
visual_score = 0.20

# Fallback to higher confidence modality
result = "goodbye moon"  âœ“
```

---

## Usage & Deployment

### Running in Replit (Demo Mode)

**Limitation**: No webcam/microphone access in cloud environment.

**Available**: Fusion algorithm demonstration
```bash
python test_fusion.py
```

**Output**:
```
Visual: 'bat' | Audio: 'pat' => Fused: 'pat' âœ“
Visual: 'fan' | Audio: 'van' => Fused: 'van' âœ“
Visual: 'see' | Audio: 'she' => Fused: 'she' âœ“
```

### Running Locally (Full System)

#### System Requirements:
- **OS**: Linux, macOS, Windows (with WSL)
- **GPU**: CUDA-capable GPU recommended (CPU also supported)
- **RAM**: Minimum 8GB
- **Storage**: 3GB for models
- **Hardware**: Webcam + Microphone

#### Installation:

```bash
# 1. Clone/download project
git clone <repository_url>
cd audio-visual-lipnet

# 2. Install system dependencies (Linux/Ubuntu)
sudo apt-get install portaudio19-dev

# 3. Install Python packages
pip install -r requirements.txt

# 4. Install Ollama
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3.2

# 5. Run the application
python main_with_audio.py
```

#### Controls:
- **Alt**: Start/Stop recording
- **Q**: Quit application

#### Usage Flow:
1. Application opens webcam window
2. Face the camera, ensure good lighting
3. Press **Alt** to begin recording
4. Speak clearly while looking at camera
5. Press **Alt** to stop recording
6. System displays:
   - Visual prediction
   - Audio prediction
   - Fused result
   - LLM-corrected output
7. Repeat or press **Q** to quit

### Visual-Only Mode (Original System)

To run the original lip-reading system without audio:
```bash
python main.py
```

This uses only the visual speech recognition pipeline.

---

## Limitations & Future Work

### Current Limitations

#### 1. Global Confidence Scores
**Issue**: The fusion algorithm uses utterance-level confidence scores rather than word-level or phoneme-level scores.

**Impact**: 
- When predictions conflict, entire utterance goes to higher-weighted modality
- Cannot mix words from different modalities intelligently
- Loses granular information about per-word reliability

**Example**:
```python
Visual: "the quick brown fox" (confidence: 0.5)
Audio:  "the slow red dog" (confidence: 0.7)

Current: "the slow red dog"  # Entire audio prediction
Ideal:   "the quick brown fox"  # Mix based on per-word confidence
```

#### 2. Visual Confidence Not Extracted
**Issue**: Visual model doesn't return actual confidence scores.

**Current Workaround**: Defaults to 0.5
**Impact**: Cannot accurately weight visual predictions

#### 3. No Temporal Alignment
**Issue**: Audio and visual streams are not aligned at word level.

**Impact**: 
- Timing mismatch between modalities
- Word-level fusion is approximate, not precise

#### 4. Limited Language Model Integration
**Issue**: LLM post-processing is separate, not integrated into fusion.

**Impact**: Cannot use language model to rescore and select best combination

#### 5. Environment Constraints
**Hardware Required**:
- Physical webcam (no remote/cloud support)
- Physical microphone
- Cannot run in Replit or cloud environments

### Recommended Improvements

#### Phase 1: Enhanced Confidence Extraction
```python
# Extract per-token confidence from visual model
visual_result = {
    'tokens': ['the', 'cat', 'sat'],
    'confidences': [0.9, 0.6, 0.8],
    'global_confidence': 0.77
}

# Extract from audio model
audio_result = {
    'tokens': ['the', 'bat', 'sat'],
    'confidences': [0.95, 0.7, 0.85],
    'global_confidence': 0.83
}
```

#### Phase 2: Token-Level Fusion
```python
def token_level_fusion(visual_result, audio_result):
    """Fuse at word/phoneme level"""
    fused_tokens = []
    
    # Align tokens temporally
    aligned = align_sequences(
        visual_result['tokens'],
        audio_result['tokens']
    )
    
    # Choose best token for each position
    for v_token, a_token in aligned:
        v_conf = visual_result['confidences'][v_token.index]
        a_conf = audio_result['confidences'][a_token.index]
        
        weighted_v = v_conf * 0.4
        weighted_a = a_conf * 0.6
        
        best_token = a_token if weighted_a > weighted_v else v_token
        fused_tokens.append(best_token)
    
    return fused_tokens
```

#### Phase 3: Language Model Rescoring
```python
# Generate multiple hypotheses
hypotheses = [
    "the cat sat",  # Visual-only
    "the bat sat",  # Audio-only  
    "the cat sat",  # Fused
]

# Rescore using language model
lm_scores = language_model.score(hypotheses)

# Select best based on combined score
final = max(hypotheses, key=lambda h: 
    fusion_score(h) + lm_weight * lm_scores[h]
)
```

#### Phase 4: Learned Fusion Weights
```python
# Train neural network to learn optimal fusion
class LearnedFusion(nn.Module):
    def __init__(self):
        self.attention = nn.MultiheadAttention(512, 8)
        self.fusion_net = nn.Sequential(
            nn.Linear(1024, 256),
            nn.ReLU(),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )
    
    def forward(self, visual_features, audio_features):
        # Attend to relevant features
        attended_v = self.attention(visual_features)
        attended_a = self.attention(audio_features)
        
        # Learn fusion weight
        combined = torch.cat([attended_v, attended_a], dim=-1)
        weight = self.fusion_net(combined)
        
        # Weighted combination
        fused = weight * audio_features + (1 - weight) * visual_features
        return fused
```

#### Phase 5: Temporal Alignment
```python
# Dynamic Time Warping for audio-visual sync
from scipy.spatial.distance import euclidean
from fastdtw import fastdtw

def align_modalities(visual_sequence, audio_sequence):
    """Align audio and visual streams"""
    distance, path = fastdtw(
        visual_sequence,
        audio_sequence,
        dist=euclidean
    )
    
    # Map timestamps between modalities
    alignment = [(v_idx, a_idx) for v_idx, a_idx in path]
    return alignment
```

### Future Enhancements

1. **Whisper Integration**: Replace Google Speech API with local Whisper model
   - Privacy (offline processing)
   - Better accuracy
   - Confidence scores per word

2. **Noise-Adaptive Fusion**: Automatic weight adjustment
   - Estimate audio SNR (signal-to-noise ratio)
   - Increase visual weight in noisy environments
   - Increase audio weight in low-light conditions

3. **Multi-Speaker Support**: Handle multiple people
   - Face tracking with speaker identification
   - Per-speaker audio source separation
   - Individual fusion streams

4. **Real-Time Streaming**: Incremental processing
   - Online inference (word-by-word)
   - Low-latency output
   - Continuous streaming mode

5. **Production API**: Deployable service
   - REST API endpoints
   - Batch processing support
   - Model serving with TorchServe

---

## Model Performance

### LRS3 Benchmark Results

**Dataset**: LRS3 (433 hours of TED/TEDx talks)
**Model**: Transformer-based VSR
**Configuration**: `LRS3_V_WER19.1`

| Metric | Value |
|--------|-------|
| Word Error Rate (WER) | 19.1% |
| Character Error Rate (CER) | ~8% |
| Model Size | 1.77 GB |
| Inference Speed (GPU) | ~0.5s per utterance |
| Inference Speed (CPU) | ~2s per utterance |

### Fusion Performance (Expected)

Based on typical multimodal fusion results:

| Condition | Visual WER | Audio WER | Fused WER |
|-----------|-----------|-----------|-----------|
| Clean (no noise) | 19.1% | 5% | **3-4%** âœ“ |
| Moderate noise | 19.1% | 15% | **12-14%** âœ“ |
| High noise | 19.1% | 40% | **18-20%** âœ“ |
| Low light | 35% | 5% | **6-8%** âœ“ |

**Key Insight**: Fusion provides robustness - when one modality degrades, the other compensates.

---

## Dependencies

### Python Packages
```
hydra-core >= 1.3.2          # Configuration management
opencv-python >= 4.5.5       # Computer vision
torch >= 2.0.0               # Deep learning framework
torchvision >= 0.15.0        # Vision models
torchaudio >= 2.0.0          # Audio processing
scipy >= 1.3.0               # Scientific computing
scikit-image >= 0.13.0       # Image processing
av >= 10.0.0                 # Video I/O
mediapipe >= 0.10.14         # Face detection
ollama >= 0.6.0              # LLM integration
pydantic >= 2.0.0            # Data validation
keyboard >= 0.13.5           # Hotkey controls
SpeechRecognition >= 3.10.0  # Audio speech recognition
sounddevice >= 0.4.6         # Audio capture
numpy >= 1.24.0              # Numerical computing
```

### System Dependencies
```
portaudio19-dev              # Audio I/O library
ffmpeg                       # Video processing
libsndfile1                  # Audio file I/O
```

---

## Project Files

### Documentation
- `README.md`: User-facing guide with quick start
- `INSTALLATION.md`: Detailed local setup instructions  
- `PROJECT_SUMMARY.md`: This comprehensive technical overview
- `replit.md`: Project memory and architecture notes

### Source Code
- `fusion.py`: Multimodal fusion algorithm (hardware-independent)
- `audio_module.py`: Audio capture and speech recognition
- `main_with_audio.py`: Enhanced application with audio integration
- `main.py`: Original visual-only lip-reading system
- `test_fusion.py`: Fusion algorithm demo (runs in Replit)

### Configuration
- `requirements.txt`: Python dependencies
- `configs/*.ini`: Model configurations
- `hydra_configs/default.yaml`: Runtime settings
- `.gitignore`: Version control configuration

---

## Acknowledgments

- **ESPnet**: Open-source speech processing toolkit
- **LRS3 Dataset**: University of Oxford lip-reading dataset
- **Ollama**: Local LLM inference engine
- **MediaPipe**: Google's ML solutions for face detection

---

## License & Usage

This project is for educational and research purposes. Pre-trained models are subject to their original licenses (ESPnet, LRS3).

---

**Last Updated**: November 9, 2025
**Version**: 1.0 (Audio Integration Complete)
